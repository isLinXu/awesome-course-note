# Lecture4

这份文档是哈佛大学CS197课程的第四讲笔记，主题为“AI Research Experiences”，由Pranav Rajpurkar主讲。以下是对文档内容的详细解释和分析，以及相应的课程笔记。

### 课程概述
- **目标**：通过实践学习AI/ML工程，特别是深度学习。
- **方法**：使用Huggingface库，专注于自然语言处理（NLP），特别是大型语言模型。
- **结构**：本讲为实时编码演示，介绍如何微调预训练的语言模型。

### 学习成果
- **数据集处理**：使用datasets库加载和处理NLP数据集。
- **文本序列化**：了解文本序列化（tokenization）的步骤。
- **构建模型**：构建用于因果语言建模的数据集和训练步骤。

### 微调语言模型
- **语言模型**：预测句子中的单词，重点介绍因果语言建模（CLM），即仅使用序列中之前的token来预测下一个token。
- **Huggingface**：一个用于构建、训练和部署基于开源软件的ML模型的社区和数据科学中心。

### 加载数据集
- **Datasets库**：用于高效加载和处理数据，访问和共享数据集，与DL框架互操作。
- **SQuAD数据集**：斯坦福问答数据集，包含基于Wikipedia文章的问题，答案为文本段落或不可答。

### 文本序列化（Tokenization）
- **序列化步骤**：包括标准化、预标记化、模型运行和后处理。
- **子词标记算法**：BPE、WordPiece和Unigram等。
- **AutoTokenizer类**：根据模型名称实例化标记器，确保使用与模型预训练时相同的规则。

### 数据处理
- **因果语言建模**：将不同示例连接起来，然后分成大小相等的块，以便所有示例具有相同长度，无需填充。

### 模型训练
- **训练参数**：定义训练参数，设置Trainer类。
- **评估模型**：使用困惑度（perplexity）作为指标，较低的困惑度表示模型性能更好。

### 模型上传
- **Hub平台**：Huggingface的平台，用于分享和探索模型、数据集和演示。

### 模型生成
- **微调模型**：使用微调模型自动完成问题。

### 练习
1. **从头开始**：不使用预训练模型，从头开始训练模型。
2. **更换模型**：将DistilGPT替换为非GPT因果语言模型。
3. **更换数据集**：将SQuAD数据集替换为其他数据集。

### 课程笔记
1. **实践学习**：通过构建和工程实践学习AI/ML工程。
2. **Huggingface库**：用于NLP任务的流行工具集。
3. **数据集加载**：使用Datasets库高效处理数据集。
4. **Tokenization**：了解文本序列化的过程和算法。
5. **因果语言建模**：构建用于CLM的数据集和训练步骤。
6. **模型训练**：定义训练参数，使用Trainer类进行模型训练。
7. **模型评估**：使用困惑度评估模型性能。
8. **模型上传**：将模型上传到Huggingface的Hub平台。
9. **生成任务**：使用微调模型进行文本生成。
10. **练习**：通过练习加深对模型训练和生成任务的理解。

通过这些笔记，学生应该能够对如何使用Huggingface库进行NLP任务有一个基本的了解，并能够开始应用这些工具来提高他们的AI工程技能。