# Lecture14&15

这份文档是哈佛大学CS197课程的第十四和第十五讲笔记，主题为“AI Research Experiences”，由Pranav Rajpurkar主讲。以下是对文档内容的详细解释和分析，以及相应的课程笔记。

### 课程概述
- **目标**：学习如何在AWS EC2实例上设置、优化和进行深度学习训练，特别使用CheXzero代码库。
- **重点**：AWS设置、连接实例、调整代码以使用GPU、加速模型训练的策略、以及实际操作训练过程。

### 学习成果
- **AWS EC2设置**：了解如何设置和连接AWS EC2实例进行深度学习。
- **GPU使用**：学会修改深度学习代码以使用GPU。
- **实践训练**：获得使用真实代码库运行模型训练过程的实践经验。

### AWS设置
- **EC2实例**：使用Amazon EC2（Elastic Compute Cloud）创建和管理虚拟机实例。
- **实例创建**：选择适当的AMI（Amazon Machine Image）和实例类型，创建密钥对以进行身份验证。

### 连接到实例
- **SSH配置**：修改本地`.ssh/config`文件以连接到AWS实例。
- **实例启动**：使用AWS EC2终端启动实例。

### 实用工具
- **.screenrc**：终端配置文件，提供额外的功能，如粗体颜色。
- **Zsh**：提供自动cd、递归路径扩展和插件及主题支持的bash替代品。

### 转换为使用GPU
- **环境设置**：激活预安装的PyTorch环境。
- **代码设置**：修改代码以包括计时，并添加Wandb日志记录。

### GPU调整
- **CUDA检查**：确保CUDA可用，并相应地定义设备。
- **模型和数据加载**：将模型和数据加载到GPU。

### 加速训练
- **更多GPU**：如果使用完所有GPU容量，可能需要选择具有更多GPU的实例类型。
- **增加批量大小**：尝试增加批量大小以使用更多的GPU存储。
- **增加工作线程数**：增加工作线程数以加快数据加载速度。

### CheXzero实践
- **安装CheXzero**：克隆GitHub仓库并安装Python依赖。
- **数据集访问**：获取MIMIC-CXR数据库并开始下载。
- **预处理数据**：使用`pre_process.py`处理数据并生成HDF5文件。
- **探索HDF5文件**：使用Jupyter笔记本检查HDF5文件内容。
- **训练模型**：运行训练脚本并解决可能出现的错误。

### 课程笔记
1. **AWS EC2实例**：学习如何在AWS上创建和管理EC2实例。
2. **实例连接**：了解如何通过SSH连接到EC2实例。
3. **终端配置**：使用`.screenrc`和Zsh提高终端使用体验。
4. **GPU利用**：调整代码以在GPU上运行，显著加速计算过程。
5. **性能优化**：探讨使用更多GPU、增加批量大小和增加工作线程数来提升训练速度。
6. **CheXzero代码库**：实际操作CheXzero代码库，进行模型训练和零样本评估。
7. **数据预处理**：了解如何预处理数据并使用HDF5文件格式。
8. **错误解决**：学习如何解决常见的深度学习训练中的错误。

通过这些笔记，学生应该能够对如何在AWS EC2实例上进行深度学习训练有一个基本的了解，并能够开始应用这些技能来提高他们的模型训练效率。