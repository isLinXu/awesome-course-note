# Lecture10

这份文档是关于“可信机器学习：大型语言模型及应用”的课程，第10讲的PDF笔记，由Eric Mitchell在2023年10月26日所作的客座讲座。以下是对文档内容的详细解释和分析：

### 1. 概览
- **主题**: 提高大型语言模型（LLMs）的事实性。
- **内容**: 讨论了LLMs在事实性方面的问题，简要介绍了Transformers，并探讨了提高LLMs事实性的方法。

### 2. LLMs的事实性问题
- **问题**: LLMs能够生成令人信服但并不总是正确的内容。
- **例子**: 引用了关于谷歌AI聊天机器人Bard和“新Bing”在处理事实性问题时出现错误的案例。

### 3. Transformers概述
- **Transformers**: 介绍了Transformers的基本工作原理，包括嵌入步骤、自注意力机制、多层感知机（MLP）和残差连接。
- **自回归Transformer**: 解释了自回归Transformer如何通过预测下一个词/像素/标记来生成文本。

### 4. 提高LLMs的事实性
- **当前LLMs的问题**: 当前LLMs不能被信任，需要探索提高其事实性的方法。
- **问题**: 讨论了如何确定LLMs是否能够对事实性进行编码。

### 5. LLMs是否能够编码事实性
- **问题**: 探讨了LLMs是否能够对语句的真实性进行二元解码，以及LLMs是否提供了校准的不确定性。

### 6. 解码语句的真实性
- **策略**: 提出了两种策略来解码LLMs隐藏状态中语句的真实性：有监督学习和无监督学习。
- **研究**: 引用了Burns等人（2023年）的研究，该研究表明可以通过无监督探针来预测语句的真实性，而无需任何标记数据。

### 7. 模型置信度与事实性
- **研究**: Kadavath等人（2022年）研究了模型校准问题，即LLMs的置信度是否反映了答案实际正确的概率。
- **发现**: 更大的LLMs在校准方面表现得越来越好。

### 8. 模型不确定性与事实性
- **研究**: Kuhn等人（2022年）探讨了模型不确定性（如预测熵）是否能够预测真实性。

### 9. 训练LLMs以提高事实性
- **RLHF**: 介绍了强化学习从人类反馈（RLHF）的方法，包括学习奖励模型和优化策略。
- **问题**: 讨论了为什么RLHF可能不足以提高事实性，以及如何通过自动化的事实性排名来改进LLMs。

### 10. 训练LLMs的具体方法
- **方法**: Tian等人（2023年）提出了使用参考基础的事实性评分（FactScore）来训练LLMs。
- **评估**: 在长篇生成任务上评估了事实性调整的效果。

### 11. 结论
- **挑战**: 构建能够产生事实性输出的系统是NLP领域的一个关键挑战。
- **原因**: LLMs拥有关于真假的内部模型，可以通过解码来预测真假，并且能够产生校准的概率。
- **进展**: 使用自动化事实性排名的强化学习可以提高事实性。
- **未来工作**: 鼓励研究者继续在提高LLMs的事实性和鲁棒性方面进行工作。

### 12. 联系方式
- **联系**: Eric Mitchell提供了他的联系方式，以便于有兴趣的人进行交流和提问。

这份笔记为听众提供了关于如何提高LLMs事实性的深入分析，并探讨了当前的挑战和潜在的解决方案。通过这些内容，听众可以更好地理解LLMs在生成事实性内容方面的潜力和局限性。