# Lecture2

这份PPT是关于卷积神经网络（CNN）和循环神经网络（RNN）的课程介绍，由Colby Wang和Dongfu Jiang在2023年12月29日为加拿大滑铁卢大学（UWaterloo）的CS886课程准备。以下是对PPT内容的详细解释和分析，以及相应的课程笔记：

### 课程内容概览
- **循环神经网络基础**：介绍RNN的基本概念，包括LSTM和GRU等变体，以及RNN在不同下游应用中的使用。
- **卷积神经网络基础**：介绍CNN的基本概念，包括ConvNext、ResNet、ResNext、DenseNet、UNet、MobileNet等，以及CNN在不同下游应用中的使用。

### 循环神经网络（RNN）
- **RNN基础**：RNN是一种处理序列数据的神经网络，通过在序列的每个时间步上应用相同的权重，并在隐藏状态中传递信息来捕捉时间动态。
- **数学公式**：RNN的隐藏状态`ht`是基于前一时间步的隐藏状态`ht-1`和当前输入`xt`的加权和计算得出的。
- **Vanilla RNN问题**：包括梯度消失和爆炸问题，这会导致模型难以学习长距离依赖。
- **长短期记忆网络（LSTM）**：LSTM通过特殊的门控结构来控制记忆和遗忘，以缓解梯度消失问题，并促进长期记忆。
- **LSTM数学公式**：包括输入门、遗忘门、输出门和候选值向量的计算。
- **LSTM门控作用**：输入门控制新值流入细胞状态的程度，遗忘门决定从细胞状态中丢弃的信息，输出门调节从细胞状态输出的信息量。
- **高速公路网络（Highway Networks）**：灵感来自LSTM，通过学习的门控机制调节信息流，以缓解深度神经网络训练中的梯度消失问题。
- **门控循环单元（GRU）**：简化版的LSTM，没有细胞状态，只有两个门（重置门和更新门），参数更少，训练更快。
- **LSTM与GRU比较**：LSTM更复杂，参数更多，适合长序列数据；GRU更简单，训练更快，适合小数据集。
- **RNN下游应用**：如机器翻译，RNN结合注意力机制可以提高翻译质量。

### 卷积神经网络（CNN）
- **CNN操作**：包括卷积操作和池化操作，用于提取图像特征。
- **LeNet-5**：第一个CNN网络，使用sigmoid激活和平均池化。
- **ImageNet**：一个超过1500万个带标签的高分辨率图像的数据集。
- **AlexNet**：第一个在GPU上训练的深度CNN网络，解决了大数据集和计算资源的问题。
- **VGGNet**：通过堆叠相同的块来增加网络深度，证明了更深的网络可以带来更好的性能。
- **ResNet**：引入残差连接来解决深层网络性能退化问题，使网络可以训练得更深。
- **DenseNet**：通过在所有层之间添加残差连接来提高性能，同时减少参数数量和计算资源。
- **GoogLeNet/Inception**：通过“分割-转换-合并”策略使网络更宽，而不是更深。
- **ResNeXt**：结合了ResNet和Inception的思想，通过应用相同的拓扑分支来开发更宽的网络。
- **ConvNeXt**：结合了现代设计技术，证明了CNN可以与ViT（视觉Transformer）模型相媲美。

### CNN下游应用
- **U-Net**：用于图像分割的全卷积网络，通过上卷积（up-convolution）来增加特征图的大小。
- **MobileNet**：通过深度可分离卷积来提高网络效率，减少计算成本。
- **其他CNN变体**：如SqueezeNet，针对下游任务（如目标检测）进行了优化的更小、更高效的CNN网络。

### 总结
这份PPT提供了对RNN和CNN的全面介绍，包括它们的基本工作原理、变体、数学公式、优势、挑战以及在各种下游任务中的应用。通过这些笔记，学生可以更好地理解这些网络的内部工作机制，以及如何选择和应用它们来解决特定的问题。