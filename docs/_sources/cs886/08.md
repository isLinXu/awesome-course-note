# Lecture8

这份PPT是关于大型语言模型（Large Language Models，简称LLMs）的课程介绍，由Ayinde Yakubu和Jerry Gu在2024年1月17日为加拿大滑铁卢大学（UWaterloo）的CS886课程准备。以下是对PPT内容的详细解释和分析，以及相应的课程笔记：

### 大型语言模型（LLM）概述
- **定义**：能够理解和生成人类语言的计算模型。
- **应用**：在大量数据集上训练，用于识别、翻译、预测或生成文本或其他内容。
- **特点**：是无监督的多任务学习者。

### LLM与传统机器学习、深度学习的比较
- **训练数据规模**：LLM需要非常大的数据集。
- **特征工程**：LLM自动完成，不需要手动特征工程。
- **模型复杂性**：LLM最为复杂。
- **可解释性**：LLM的可解释性较差。
- **性能**：LLM性能最高。
- **硬件需求**：LLM对硬件的需求非常高。

### 自然语言处理任务
- **自然语言理解**：情感分析、文本分类、自然语言推理（NLI）、语义理解、推理等。
- **自然语言生成**：摘要、问答等。

### T5框架
- **统一方法**：将各种任务统一为文本到文本的转换。
- **架构**：使用word2vec生成输入序列中每个标记的数值表示向量。

### T5性能和架构变体
- **性能**：在多个基准测试中的表现。
- **架构变体**：不同的注意力掩蔽模式。

### T5输入数据
- **数据集**：Colossal Clean Crawled Corpus，每月从网页提取20TB文本数据。

### T5训练
- **任务**：所有任务都表述为文本到文本的任务。
- **预训练步骤**：使用最大序列长度512和批量大小128序列。

### T5无监督目标
- **机制**：模型通过大量未标记文本数据集学习通用知识。

### T5预训练数据集
- **腐败策略**：包括掩蔽、替换跨度、删除和打乱等。

### T5性能结果
- **数据集大小**：不同大小的数据集对模型性能的影响。

### T5扩展
- **计算能力**：增加计算能力可以提高模型性能。

### T5反思
- **文本到文本**：提供了一种简单的方式，使用相同的损失函数和解码程序训练单一模型完成多种任务。

### 上下文学习
- **限制**：需要大型数据集，但实践中并不总是可行。

### 语言模型元学习
- **能力**：在训练期间发展广泛的技能和模式识别能力。

### GPT-3架构和训练方法
- **数据集**：基于原始Common Crawl数据集，最多1T单词。

### GPT-3训练数据集
- **清理**：通过多种方式清理原始数据集。

### GPT-3计算消耗
- **细节**：模型大小、层数、dmodel、nheads、dhead、批量大小和学习率。

### GPT-3限制
- **文本合成**：存在结构和算法限制。
- **样本效率**：预训练期间样本效率低下。
- **可解释性**：缺乏可解释性。
- **偏见和不公平**：可能会延续和放大社会中现有的偏见和不公平。

### CodeX介绍
- **进展**：在从未明确训练过代码的语言模型中生成程序。

### CodeX评估
- **方法**：使用功能正确性而不是基于匹配的指标。

### CodeX训练
- **细节**：在GitHub上的159GB独特Python文件数据集上微调GPT模型。

### CodeX结果
- **测试损失**：在保留的验证集上的交叉熵测试损失遵循幂律。

### CodeX比较
- **性能**：与其他模型（如GPT-NEO和GPT-J）的比较。

### CodeX-S
- **改进**：通过监督微调提高了性能。

### CodeX-D
- **生成文档字符串**：从代码生成文档字符串的CodeX版本。

### CodeX限制
- **训练效率**：训练数据总量达数亿行代码。

### Llama-2介绍
- **家族**：具有数十亿参数的预训练和微调LLM。

### Llama-2预训练
- **架构**：采用标准变换器架构。

### Llama-2预训练评估
- **基准**：与其他开源模型的比较。

### Llama 2-Chat
- **版本**：通过对齐技术进行监督微调的Llama-2版本。

### Llama 2-Chat人类偏好数据收集
- **反馈**：从人类反馈中获取人类偏好数据。

### Llama 2-Chat奖励建模
- **模型**：使用人类偏好数据训练奖励模型。

### Llama 2-Chat迭代微调
- **算法**：使用拒绝采样微调和近端策略优化（PPO）。

### Llama 2-Chat Ghost Attention (GAtt)
- **技术**：帮助控制多轮对话流程。

### Llama 2-Chat RLHF模型评估
- **质量**：在有帮助和安全提示的测试集上评估。

### Llama-2安全性
- **数据过滤**：在预训练和微调期间过滤有害数据。

### Mixtral of Experts (MoE)介绍
- **模型**：可以胜过Llama-2 70B和GPT3.5的稀疏混合专家模型。

### Mistral架构
- **版本**：Mixtral的早期版本。

### 稀疏Mixtral of Experts
- **架构**：与Mistral相同，但具有更长的上下文长度和MoE层。

### Mixtral结果
- **基准**：在多个基准测试中与Llama-2的比较。

### Mixtral-Instruct
- **版本**：在指令数据集上进行监督微调的Mixtral版本。

### Mixtral路由分析
- **分布**：使用The Pile数据集测量所选专家的分布。

### PaLM: Pathways Language Model
- **参数**：训练为具有5400亿参数的密集激活变换器语言模型。

### PaLM模型架构
- **设置**：仅解码器设置，每个时间步只能关注自身和过去的步骤。

### PaLM模型规模超参数
- **比较**：比较了不同模型规模的性能。

### PaLM训练数据集
- **组成**：代表广泛自然语言使用案例的高质量语料库。

### PaLM训练基础设施
- **TPU**：所有模型都在TPU v4 Pods上训练。

### PaLM结果
- **基准**：在29个NLP基准测试中的表现。

### PaLM: BIG-bench
- **基准**：旨在为LLM产生具有挑战性任务的协作基准。

### PaLM: 评估推理
- **算术推理**：需要多步逻辑推理的自然语言数学问题。

### PaLM: 链式思维提示
- **方法**：通过提供解决问题的逐步思维过程来提高模型的推理能力。

### PaLM: 链式思维结果
- **表现**：在不同任务上的表现。

### PaLM: 代码任务
- **文本到代码**：根据自然语言描述编写代码。

### PaLM: 翻译
- **任务**：将一种人类语言重写为另一种语言，同时保留输入的内容、语义和风格。

### PaLM: 限制
- **偏见**：在底层数据中包含和放大偏见。

### LLMs比较
- **模型名称**：包括T5、GPT-3、CodeX、Llama-2、Mixtral和PaLM的比较。

### 问题/讨论
- **无监督学习与强化学习**：哪种更可取？
- **LLM的大小**：是什么让语言模型变得“大型”？
- **语言模型的恶意使用**：语言模型是否可以被恶意使用？
- **减少社会伤害**：如何减少语言模型的滥用对社会造成的伤害？
- **消除社会隐含偏见**：如何从基础模型中移除社会隐含偏见？
- **LLMs的突现能力**：是什么导致了在LLMs中观察到的突现能力？

### 参考文献
- **引用**：列出了用于准备PPT的学术资源。

这份PPT为学生提供了大型语言模型的全面介绍，包括它们的定义、与传统机器学习和深度学习的比较、在自然语言处理任务中的应用、不同LLMs的架构和性能比较，以及这些模型的潜在限制和社会影响。通过这些笔记，学生可以更好地理解