# Lecture10

这份PPT是关于指令调整（Instruction Tuning）和强化学习（Reinforcement Learning）在大型语言模型（LLMs）中的应用的课程介绍，由Sahel Sharifymoghaddam、Hossein Mohebbi和Shivani Upadhyay在2024年3月13日为加拿大滑铁卢大学（UWaterloo）的CS886课程准备。以下是对PPT内容的详细解释和分析，以及相应的课程笔记：

### 动机
- **问题**：大型语言模型在互联网上训练，通常使用未标记数据进行下一个词预测的自监督学习。这些模型在某些NLP任务上表现良好，但在推理时遵循人类提供的指令方面表现不佳。

### 常见解决方案
- **微调（Fine-tuning）**：需要大量示例数据，且特定于任务。
- **少样本提示（Few-shot prompting）**：需要提示工程，上下文大小有限，且推断成本高。

### 提高性能的零样本学习
- **FLAN**：通过在多个任务上进行指令调整来提高模型的零样本学习能力。
- **T0**：通过多任务提示训练实现零样本任务泛化。
- **LIMA**：通过减少指令调整的数据量来提高模型与指令的对齐。

### 指令调整
- **定义**：在多个任务上对语言模型进行微调，这些任务通过指令描述，使模型学会遵循指令，并在零样本中泛化到未见任务。

### 指令调整示例
- **训练**：在多个任务上进行训练，使用自然语言任务指令。
- **零样本推断**：在没有训练数据的情况下，直接对新任务进行推断。

### FLAN（Finetuned Language Models Are Zero-shot Learners）
- **数据集**：62个NLP数据集，分为12个簇，代表12种任务类型。
- **模板**：为每个数据集编写了10个独特的模板。

### 分类与生成任务
- **基础模型**：LaMDA-PT，仅解码器，适用于自由文本生成任务。
- **分类任务**：需要选项，例如封闭的问答任务。

### 训练细节：基础模型
- **LaMDA-PT**：137B参数，预训练在网络文档、对话数据和维基百科上。

### FLAN训练细节
- **混合示例**：从所有数据集中混合示例，随机采样。

### 零样本结果
- **FLAN**：在某些任务上，FLAN缩小了与监督模型之间的差距。

### FLAN vs GPT-3
- **性能**：FLAN在一些任务上的零样本和少样本性能超过了GPT-3。

### T0概述
- **多任务训练**：使用众包创建不同的提示模板，用于多任务训练。

### 数据集准备
- **任务**：基于输出格式而非完成任务所需技能分组。

### T0 vs GPT-3
- **性能**：在所有NLI任务上，T0的零样本性能超过了GPT-3。

### T0 vs FLAN差异
- **基础模型**：FLAN使用LaMDA-PT，而T0使用LM+T5。

### LIMA（Less Is More for Alignment）
- **焦点**：在1000个精心策划的提示上，关注多样性输入风格和统一输出格式。

### LIMA数据选择
- **来源**：Stack Exchange、WikiHow和Reddit等。

### LIMA训练
- **模型**：使用LLaMA 65B作为基础模型。

### 评估方法
- **人类评估**：人类比较模型对给定提示的响应。

### 人类评估
- **LIMA**：在74%的测试提示中，LIMA的响应优于或等于相同大小的Alpaca微调模型。

### RLHF（Reinforcement Learning from Human Feedback）
- **动机**：训练语言模型以遵循带有人类反馈的指令。

### RLHF动机
- **问题**：LLM的默认行为可能导致不想要的输出，如编造事实、偏见或有毒文本。

### InstructGPT
- **方法**：使用人类反馈通过RL进行微调，以符合人类意图。

### 强化学习背景
- **分类**：监督学习、强化学习和无监督学习。

### PPO（Proximal Policy Optimization）
- **算法**：一种先进的策略基RL算法，通过限制更新幅度来增强训练稳定性。

### InstructGPT训练
- **过程**：通过筛选过程招募承包商，编译人类编写的示例数据集。

### 评估细节
- **数据集**：使用公共NLP数据集和API数据进行评估。

### 人类评估各种模型
- **比较**：InstructGPT在参数数量少100倍的情况下，与GPT-3的对齐程度更高。

### InstructGPT局限性
- **行为**：受人类承包商反馈的影响，可能存在偏见。

### RLHF问题
- **方法**：包括奖励模型拟合和使用人类偏好对语言模型进行微调的复杂过程。

### DPO（Direct Preference Optimization）
- **方法**：一种无需RL的训练语言模型的算法，直接从偏好数据中训练。

### DPO推导
- **目标**：优化策略，使其在保持与参考策略接近的同时，最大化偏好响应的可能性。

### DPO性能
- **任务**：在控制情感生成、摘要和单轮对话等任务中，DPO的性能优于或可比于PPO。

### Zephyr
- **方法**：通过蒸馏和AI反馈，对小型开放LLM进行完全对齐。

### Zephyr训练步骤
- **步骤**：包括蒸馏监督微调、AI反馈通过偏好和蒸馏直接偏好优化。

### Zephyr评估
- **结果**：在多轮对话和学术基准测试中，Zephyr-7B模型与更大模型的性能相当。

### 要点总结
- **LLMs**：训练目标与遵循人类指令不一致。
- **FLAN和T0**：使用指令调整提高零样本性能。
- **LIMA**：关注策划提示模板的多样性和质量。
- **InstructGPT**：使用RLHF通过人类评级数据集进行对齐。
- **DPO**：提供比RLHF更稳定的结果。
- **Zephyr**：利用蒸馏和AI反馈，7B模型与更大模型性能相当。

这份PPT为学生提供了如何通过指令调整和强化学习提高大型语言模型性能的全面介绍，包括它们在不同任务上的应用、训练细节、评估结果以及这些方法的局限性。通过这些笔记，学生可以更好地理解如何通过这些技术提高模型的对齐度和性能。