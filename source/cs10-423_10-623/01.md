这份PPT是关于生成性人工智能（Generative AI，简称GenAI）的课程概览，由Matt Gormley在卡内基梅隆大学计算机科学学院的机器学习系进行讲授。以下是根据PPT内容编写的详细课程笔记：

### 1. 生成性人工智能（GenAI）简介
- **目标**：开发智能机器，包括多个子目标，如感知、推理、控制/运动/操纵、规划、通信、创造力和学习。
- **生成性AI**：在所有这些子目标中都取得了进展。

### 2. 通信和语言模型
- **通信**：包括理解语言和生成语言。
- **大型语言模型（LLMs）**：在自动回归训练（给定前面的词生成下一个词）中表现出色。

### 3. 学习方式
- **传统学习**：通过参数估计进行机器学习。
- **上下文学习**：在测试时提供训练示例作为上下文，展示了通过推理进行学习的可能性。

### 4. 推理任务
- **LLMs**：意外地擅长某些推理任务，如链式思考提示（Chain-of-Though Prompting）。

### 5. 规划和控制
- **LLMs**：已被用于具身代理的基于地面的规划，例如LLMPlanner。

### 6. 多模态基础模型
- **文本到图像模型**：根据文本描述生成图像。
- **文本到音乐模型**：能够根据文本和音频样本进行条件化。

### 7. 扩展和训练数据
- **大型语言模型（LLMs）**：使用大量数据集进行训练，如“The Pile”。
- **RLHF**：使用强化学习和人类反馈（RLHF）对预训练的GPT模型进行微调。

### 8. 内存使用和分布式训练
- **内存使用**：讨论了如何使用半精度浮点数减少内存使用并加速GPU计算。
- **分布式训练**：介绍了模型并行的不同选项，如按层划分或按令牌和层划分。

### 9. 训练成本
- **时间线**：介绍了语言模型和图像生成的时间线，展示了从n-gram模型到Transformer LMs，再到最新的生成模型的发展历程。

### 10. 为什么要学习GenAI的内部工作
- **隐喻**：通过比喻解释了学习GenAI内部工作的重要性。

### 11. 生成AI是概率建模
- **概率建模**：讨论了如何使用概率模型来模拟变量之间的所有可能交互。

### 12. 循环神经网络语言模型（RNN-LMs）
- **关键思想**：将所有先前的词转换为固定长度的向量，并定义一个条件分布在该向量上的分布。

### 13. 课程主题
- **主题**：包括文本的生成模型、图像的生成模型、应用和调整基础模型、多模态基础模型、扩展、可能的问题（如安全/偏见/公平性、幻觉、对抗性攻击等）和高级主题（如正规化流、音频理解和合成、视频合成）。

### 14. 课程亮点
- **评分**：作业、测验、考试、项目和参与度的比重。
- **技术**：使用Piazza、Gradescope、Google Forms、Zoom和Panopto等工具。

### 15. 讲座和互动
- **提问**：鼓励学生在课堂上提问，或在Piazza上实时提问。

### 16. 先决条件
- **先决条件**：介绍了所需的机器学习入门知识。

### 17. 作业
- **作业**：概述了五个作业的主题和类型，包括PyTorch入门、大型语言模型、图像生成、适配器和多模态基础模型。

### 18. 项目
- **目标**：探索选择的生成建模技术，深入理解方法在现实世界应用中的作用。

### 19. 学习目标
- **能力**：列出了学生应该能够区分不同学习机制、实现基础模型、应用现有模型、适应基础模型、扩展方法、使用现有模型、分析理论属性、识别潜在陷阱和描述社会影响等能力。

### 20. 自动微分
- **反向模式**：介绍了反向模式自动微分（Backpropagation）的基本原理和计算图的概念。

### 21. 模块化自动微分（AutoDiff）
- **模块化**：讨论了模块化自动微分的优势，如代码重用、优化和错误查找。

### 22. PyTorch
- **PyTorch**：展示了如何在PyTorch中定义和使用神经网络模块。

### 23. n-Gram语言模型
- **n-Gram模型**：介绍了如何使用n-Gram模型生成类似人类语言的句子。

### 24. 循环神经网络（RNN）语言模型
- **RNN模型**：讨论了RNN模型如何通过迭代计算隐藏向量序列和输出向量序列来定义概率分布。

这份课程笔记提供了PPT中每个部分的概要和关键点，以帮助学生更好地理解和消化课程内容。


