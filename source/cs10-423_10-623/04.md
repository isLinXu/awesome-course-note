这份PPT是关于生成性人工智能（Generative AI）的课程内容，具体讲解了深度学习在视觉领域的应用，包括卷积神经网络（CNNs）和现代变换器（Transformers）的预训练与微调。以下是根据PPT内容编写的详细课程笔记：

### 1. 课程提醒
- **作业0**：涉及PyTorch和Weights & Biases的使用。
- **作业1**：关于文本的生成模型。
- **测验1**：即将到来。

### 2. 深度学习和语言建模回顾
- **自动微分（AutoDiff）**：用于计算可微函数的梯度。
- **计算图**：定义函数的另一种方式，特别是对于复杂的模型如RNN-LM和Transformer-LM。

### 3. 预训练与微调
- **深度学习的起点**：2006年，通过预训练方法推动了深度学习的发展。
- **深度网络训练**：介绍了三种训练思想，包括无监督逐层预训练和有监督微调。

### 4. 无监督预训练
- **自编码器（Auto-encoders）**：一种神经网络，通过编码器将输入编码为一个中间表示，然后通过解码器重构输入，以减少重构误差。

### 5. 变换器语言模型（Transformer Language Model）
- **生成式预训练**：使用未标记的句子作为训练数据，目标函数是观察到的句子的似然性。

### 6. 现代变换器模型
- **PaLM、Llama-1、Falcon、Llama-2、Mistral 7B**：介绍了不同变换器模型的参数、特点和训练数据。

### 7. 变换器模型的技术
- **旋转位置嵌入（RoPE）**：一种相对位置嵌入，通过旋转输入向量的子向量来实现。
- **分组查询注意力（GQA）**：使用相同的键值头对多个不同的查询头，减少参数数量。
- **滑动窗口注意力（Sliding Window Attention）**：也称为“局部注意力”，通过应用因果掩码来减少计算量和内存需求。

### 8. 计算机视觉背景
- **图像分类**：介绍了ImageNet LSVRC-2011比赛，包括数据集和任务描述。
- **特征工程**：展示了边缘检测、角点检测和尺度不变特征变换（SIFT）等图像处理技术。

### 9. 卷积神经网络（CNNs）
- **CNN架构**：Krizhevsky, Sutskever & Hinton, 2012年的工作，介绍了CNN在ImageNet LSVRC-2012比赛中的错误率。
- **卷积**：基础概念是通过在图像上滑动一个3x3的权重矩阵F，计算F和图像相应区域的内积，并用内积操作的输出替换图像中心的像素。
- **下采样（Downsampling）**：通过步长大于1的卷积来减少图像尺寸，例如平均池化（Averaging）和最大池化（Max-Pooling）。

### 10. CNN的典型架构
- **LeNet-5、AlexNet、VGG、ResNet**：展示了不同CNN架构和它们在ImageNet比赛中的表现。

### 11. CNN的训练
- **机器学习食谱**：介绍了如何使用训练数据、定义目标、选择决策函数和损失函数，并通过SGD进行训练。

### 12. CNN的可视化
- **CNN可视化工具**：提供了可视化CNN中不同层的链接，帮助理解卷积操作。

这些笔记概括了PPT中的主要概念和深度学习在视觉领域的应用，可以作为学习和理解生成性人工智能课程内容的基础。如果你对某个特定部分有疑问或需要更深入的解释，请随时提问。

