这个PDF文件是关于"Parameter Efficient Fine-Tuning"（参数高效微调）的课程讲义，由Matt Gormley在2024年2月21日于卡内基梅隆大学计算机科学学院机器学习系进行的讲座。讲义内容涵盖了以下几个主要部分：

### 1. 课程提醒
- 作业3：应用和适配大型语言模型（LLMs），提交截止日期为2月29日晚上11:59。
- 宽限日政策变更：现在有8个宽限日，可以晚交最多4天（之前是6个宽限日，晚交3天）。

### 2. 参数高效微调（PEFT）
- **大型语言模型（LLMs）的少量样本学习**：使用标准监督目标和反向传播来微调大型语言模型。
  - 优点：符合标准机器学习流程，即使样本数量N很大也能工作。
  - 缺点：反向传播需要的内存和计算时间是前向计算的大约3倍；可能无法访问模型权重（例如，模型可能是专有的）。
- **上下文学习**：将训练样本作为提示输入LLM，让模型在解码时推断训练样本中的模式。
  - 缺点：提示可能非常长，且Transformer语言模型需要O(N^2)的时间/空间，其中N是上下文的长度。
  - 优点：不需要反向传播，只需要一次通过训练数据；不需要模型权重，只需要API访问。

### 3. 监督微调与上下文学习
- 讨论了为什么即使效率较低，我们仍然需要考虑微调，因为微调通常比上下文学习表现更好。

### 4. 参数高效微调的目标和方法
- 目标：对较少的参数进行微调，但实现与微调所有参数相当的下游任务性能。
- 方法包括：
  - **子集选择**：选择参数的子集进行微调。
  - **适配器**：添加少量参数的额外层，并仅微调这些层的参数。
  - **LoRA**：为每个参数矩阵学习一个小的增量，增量选择为低秩。
  - **前缀调整**：对于Transformer语言模型，假设序列之前存在许多标记并调整对应于这些标记的键/值。

### 5. 仅微调顶层
- 作为PEFT的简单基线，只固定顶层K层的参数，减少内存使用。

### 6. 适配器（Adapters）
- 适配器层是一个带有残差连接的前馈神经网络，输入和输出维度相同，但在中间降低到较低的维度m。

### 7. 前缀调整和LoRA
- 前缀调整和LoRA是两种不同的参数高效微调方法，用于Transformer模型。

### 8. 大型语言模型（LLMs）的规模
- 列出了一些最新的大型语言模型的规模和参数数量。

### 9. 微调LLMs而不使用正则化
- 讨论了为什么在不使用正则化的情况下微调LLMs不会过拟合。

### 10. 内在维度（Intrinsic Dimensionality）
- 讨论了模型参数数量可能不是衡量学习问题所需自由度的好指标。

### 11. LoRA的动机和关键思想
- LoRA通过学习原始预训练参数的加性修改来保持原始预训练参数固定。

### 12. LoRA的应用和结果
- LoRA在GPT-3上的应用结果表明，它几乎可以达到完整微调的性能，但参数数量更少。

### 13. PEFT在Transformer和视觉Transformer中的应用
- 讨论了如何将LoRA和适配器结合使用，以及它们在计算机视觉任务中的应用。

这份讲义深入探讨了如何高效地微调大型预训练模型，特别是在参数数量庞大的情况下。它介绍了不同的微调策略，包括只微调模型的一部分、使用适配器、前缀调整和LoRA等方法，并通过实验结果展示了这些方法的有效性。此外，还讨论了大型模型的内在维度问题，以及如何通过内在维度来理解和改进微调过程。