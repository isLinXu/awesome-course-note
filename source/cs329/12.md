# Lecture12

这份文档是关于“可信机器学习：大型语言模型及应用”的课程，第11讲的PDF笔记。以下是对文档内容的详细解释和分析：

### 1. 引言
- **主题**: 解释大型语言模型（LLMs）的可解释性概念和技术。
- **现状**: LLMs的可解释性仍处于初级阶段，但有望逐渐发展。

### 2. 机器学习模型的评估
- **测试/训练分割**: 通过将数据分为训练集和测试集来评估模型性能。
- **理论上的评估**: 可能近似正确学习（PAC学习），假设训练样本是从数据分布中随机抽取的。

### 3. 测试准确性的问题
- **准确性变化**: 测试准确性可能因数据的不同部分而变化。
- **代表性**: 测试集可能无法代表部署环境，可能导致不同的影响。

### 4. 解释模型预测
- **解释类型**: 解释可以基于输入特征、神经元激活、训练样本或训练阶段。

### 5. 特征归因
- **问题陈述**: 将模型对输入的预测归因于输入的特征。
- **应用**: 调试模型预测、为最终用户生成解释、分析模型鲁棒性和监控生产中的模型。

### 6. 归因的简单方法
- **消融**: 删除每个特征并注意预测变化，但计算成本高且可能误导。
- **特征×梯度**: 将每个特征的归因视为特征值乘以其梯度。

### 7. 集成梯度（Integrated Gradients）
- **方法**: 通过从基线到输入的直线路径上的梯度积分来解释模型输出。
- **基线**: 对于图像模型，基线是黑色图像；对于文本模型，基线是空文本或零嵌入向量。

### 8. 集成梯度的应用
- **调试工作流程**: 构建模型，检查样本上的归因，如果看起来正常，则修复测试/训练分割、数据、特征、架构和目标。

### 9. 内部影响
- **影响指导的解释**: 识别影响模型属性的重要因素并使其具有人类可解释性。

### 10. 影响路径和模式
- **影响路径**: 提供对错误分类的洞察，可以压缩模型而不改变其效用。
- **影响模式**: 用于识别和可视化网络中具有影响力的神经元。

### 11. What-If探索
- **探索**: 在各种What-If场景下探索模型。
- **问题**: 如何导航What-If场景的巨大空间？

### 12. 目标What-If技术
- **技术**: 通过有序的方式迭代扰动空间，以识别实现目标预测的最小扰动。

### 13. LLM应用想法：提示扰动
- **应用**: 定义扰动空间，识别保持性能的最大化令牌扰动，以及大幅改变性能的最小令牌扰动。

### 14. 总结
- **测试准确性**: 单独的测试准确性可能是误导性的，需要检查模型在不同部分上的表现。
- **模型推理**: 检查模型是否依赖于偶然或无关的特征。
- **特征归因**: 用于发现模型行为，适当的可视化和人类思考对于将归因转化为洞察至关重要。

### 15. 调试工作流程
- **流程**: 构建模型，检查样本上的归因，如果看起来正常，则修复测试/训练分割、数据、特征、架构和目标。

### 16. 人类分析师的作用
- **人类分析师**: 人类不擅长预见问题，但擅长理解特定解释的真实世界影响。

### 17. 可视化的重要性
- **论文**: 探索深度网络归因的原则性可视化。

### 18. 特征归因的局限性
- **归因**: 不解释网络如何结合特征产生答案，或训练数据如何影响预测。

### 19. 评估集成梯度
- **评估**: 通过消除顶部归因特征并检查预测变化，或将归因与人类提供的关于“特征重要性”的地面真实进行比较。

### 20. 集成梯度的公理化证明
- **公理**: 为归因方法列出理想的标准（公理），并建立唯一性结果。

这份笔记为听众提供了关于如何解释和评估LLMs的深入分析，并探讨了当前的挑战和潜在的解决方案。通过这些内容，听众可以更好地理解LLMs在生成可解释性内容方面的潜力和局限性。