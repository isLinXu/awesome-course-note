# Lecture4

这份PPT是关于自注意力（Self-Attention）和变换器（Transformers）的课程介绍，由Evelien Riddell和James Riddell在2024年3月31日为加拿大滑铁卢大学（UWaterloo）的CS886课程准备。以下是对PPT内容的详细解释和分析，以及相应的课程笔记：

### 自注意力和变换器概述
- **自注意力**：是一种机制，允许模型在给定步骤中关注输入中最相关或最重要的部分，特别适用于序列到序列建模。
- **变换器**：是一种基于自注意力机制的架构，它在自然语言处理（NLP）和计算机视觉（CV）中都有应用。

### 注意力机制的背景
- 在变换器之前，注意力机制已经与循环神经网络（RNN）结合使用，例如在机器翻译和图像字幕生成中。
- 注意力允许模型利用上下文信息，这对于图像（空间局部性）和序列数据（时间局部性）都是有益的。

### 学习注意力机制
- 注意力机制通常指的是一个函数，它允许模型关注不同的内容。
- 注意力机制有多种形式，如加性注意力和点积注意力。

### 序列到序列学习
- 序列到序列学习是处理文本到文本（例如问答、翻译、文本摘要）和图像到文本（例如图像字幕）任务的一种方法。
- 通常使用编码器-解码器模型，如RNN或变换器。

### RNN和注意力结合
- RNN结合注意力机制可以改善序列到序列学习，尤其是在机器翻译任务中。
- 注意力权重（通过softmax函数规范化的对齐分数）允许模型在解码器的每一步关注编码器的不同部分。

### 图像字幕生成与视觉注意力
- 类似地，注意力机制可以用于图像字幕生成任务，其中模型为生成的每个单词关注图像的不同区域。

### 注意力就是一切（Attention Is All You Need）
- 该论文提出了将注意力机制与RNN解耦，并使用自注意力来提高效率。
- 贡献包括多头注意力和变换器架构。

### 变换器架构
- 变换器使用自注意力在编码器中，允许模型关注前一个编码器层的所有位置。
- 在解码器中使用掩蔽自注意力，以自回归方式处理，防止在训练期间看到未来信息。

### 变换器的误解
- 注意力在变换器中执行向量相似性搜索的概念是一种过度简化的说法。
- 变换器的关键在于学习如何基于上下文确定权重，以指示要关注的元素。

### 多头注意力
- 多头注意力是缩放点积注意力的扩展，它利用多个头来关注不同的事物。
- 多头注意力允许模型在不同的表示子空间中同时关注信息，类似于集成学习。

### 变换器架构详解
- 变换器的编码器和解码器层由多个子层组成，包括多头自注意力、馈前向网络和层归一化。
- 编码器层和解码器层的主要区别在于解码器层包括与编码器输出的交叉注意力，并且自注意力被掩蔽以防止向前看偏差。

### 变换器的组装
- 变换器通过堆叠多个编码器和解码器层，然后在顶部添加一个生成器（预测头）来组装。
- 生成器是一个线性映射，将内部表示转换为序列中下一个元素的最大似然的对数几率。

### 变换器的训练
- 模型的属性取决于它如何被训练，而不仅仅是它的架构。
- 训练可以影响模型参数，而架构和训练共同定义了模型。

### 变换器的影响
- 变换器架构对NLP领域产生了深远的影响，成为许多最新模型的基础，并且是所有大型语言模型（LLMs）的基本构件。

### 图像变换器和视觉变换器
- 图像变换器受到文本中变换器架构的启发，但主要用于局部自注意力机制。
- 视觉变换器（ViT）将图像分割成小块，并将这些块作为序列处理，应用标准的变换器编码器进行图像分类。

### 讨论
- 课程最后提供了讨论环节，以促进对变换器模型和自注意力机制的深入理解。

### 附录
- 提供了《The Illustrated Transformer》的链接，这是一个可视化变换器的交互式解释，有助于理解变换器的工作原理。

这份PPT为学生提供了变换器和自注意力机制的全面介绍，包括它们的历史、工作原理、架构、以及在NLP和CV中的应用。通过这些笔记，学生可以更好地理解这些概念，并为进一步的学习打下坚实的基础。