# Lecture11

这份PPT是关于大型语言模型（LLM）的高效推理（Inference）的课程介绍，由Rushabh Nikhilkumar Solanki和Noble Saji Mathews在2024年3月29日为加拿大滑铁卢大学（UWaterloo）的CS886课程准备。以下是对PPT内容的详细解释和分析，以及相应的课程笔记：

### 高效LLM推理概述
- **早期退出机制**：包括基于耐心的退出和自信的自适应语言建模。
- **并行推理**：探讨了推测性采样和前瞻性解码。
- **优化的注意力算法**：包括高效内存管理的分页注意力和长上下文推理的Flash解码。

### 预备论文
- 讨论了浅层-深层网络和BranchyNet，这些模型通过在深层神经网络中提前退出来加速推理。

### 标准LM推理
- 介绍了常规语言模型推理的过程。

### “过度思考”
- 通过在DNN的前向传递中附加内部分类器（ICs），可以观察到每个预测是如何演变的。

### 高效LM推理
- 介绍了如何通过在模型中设置置信度阈值来实现早期退出，从而提高推理效率。

### Shallow-Deep Networks | BranchyNet
- 这些模型使用预测概率或预测熵分数作为早期退出的置信度度量。

### 明显的缺陷
- 高置信度分数并不一定意味着分类器正确的概率更高。

### 耐心基早期退出（PABEE）
- PABEE使用耐心作为早期退出的指标。

### PABEE的准确性和速度
- 在GLUE基准测试中评估了所提出的方法，并展示了ALBERT模型的速度提升。

### 准确性提升？
- PABEE机制可以在不降低准确性的情况下提高推理速度。

### 防御对抗性攻击
- PABEE在NLI任务上的防御能力是原始ALBERT的3倍以上。

### 局限性
- PABEE仅适用于具有单个分支的模型，并且其有效性取决于耐心超参数的选择。

### 自信的自适应语言模型
- 一个动态分配不同计算量的框架，基于输入和生成时间步。

### 早期退出的影响
- 讨论了在解码过程中如何计算隐藏状态。

### 状态传播预言机
- 使用预言机置信度度量，仅在最早层级与顶级预测一致时退出。

### 对局部错误的敏感性
- 讨论了模型对早期时间步骤中的扰动的敏感性。

### 处理早期局部错误
- 使用每层阈值并为早期时间步骤设置更高的阈值。

### 衰减阈值
- 根据采样扰动中的对数行为，使用具有用户定义温度的指数函数。

### 局部置信度度量
- 作者尝试了三种置信度度量：Softmax响应、隐藏状态饱和和早期退出分类器。

### 原则性方法
- 开发了一种校准局部、每令牌退出决策的方法，以保持全局序列级约束。

### 全局约束
- 讨论了文本一致性和风险一致性的概念。

### 校准早期退出
- 使用基本配方来指定可能产生可接受生成的候选分类器退出阈值的网格。

### 性能-效率权衡
- 使用T5X框架进行实验，并比较了三种置信度度量在三个任务上的性能。

### 执行示例
- CALM通过在可能的情况下提前退出来加速生成，并仅在少数令牌上选择性地使用完整解码器的容量。

### 局限性
- 需要选择合适的置信度度量，而最有效的选项（基于Softmax的）可能计算成本高昂。

### 推测性解码
- 一种算法，通过并行计算多个令牌来加速从自回归模型中采样，而无需更改输出。

### 推测性执行
- 使用近似模型（GPT类模型）顺序执行，目标模型（T5-XXL模型）并行执行。

### 错误的推测
- 展示了目标模型并行执行时可能出现的错误推测。

### 纠正路径
- 展示了如何通过目标模型并行执行来纠正路径。

### 推测性采样
- 引入了一种新的采样方法，即推测性采样，旨在最大化这些推测任务被接受的概率。

### 关键观察
- 讨论了语言建模任务中的子任务，以及如何使用更高效的模型进行近似。

### 权衡
- 讨论了草图模型（M_q）与目标模型（M_p）的相似性与运行成本之间的权衡。

### 墙时改进
- 展示了简化的完整编码器-解码器变换器栈的跟踪图。

### 选择最优γ
- 讨论了如何找到最大化墙时的最优γ。

### 加速
- 测试了不同大小的T5模型，并展示了它们在不同任务上的速度提升。

### 局限性
- 通过增加并发性来提高延迟，但这是以增加算术运算数量为代价的。

### 高效LLMs是全栈问题
- 讨论了LLM推理和服务的库，以及如何最大限度地利用每个GPU。

### LLM基于应用的兴起
- 需要更高效的方式来处理LLM请求。

### 常规推理的分解
- 讨论了解码器的重复计算和高效内存管理的必要性。

### 管理KV缓存
- 讨论了内存碎片化和共享内存的问题。

### 按需内存
- 介绍了按需内存的概念，以及如何使用虚拟内存和物理内存。

### 多请求
- 展示了如何处理多个请求并避免外部碎片化。

### 解码技术 - 共享KV块
- 讨论了如何通过共享KV块来提高解码效率。

### 束搜索 - 派生、追加和释放
- 支持动态块映射和写时复制。

### 分页注意力
- 讨论了如何通过分页注意力来管理内存使用。

### 结果
- 分页注意力可以有效地管理内存使用，从而实现更高的吞吐量。

### Flash-Attention（重新访问）
- 讨论了Flash Attention在训练和推理中的应用。

### Flash-Decoding（PyTorch）
- 介绍了如何在长序列推理期间加速注意力计算。

### 基准测试 - CodeLLama 34B
- 展示了Flash-Decoding在解码速度上的性能提升。

### 预期解码
- 探讨了如何通过查看提示来提高令牌接受率。

### 雅可比迭代
- 讨论了并行令牌生成和非线性雅可比过程。

### 我们可以做得更好吗？
- 提出了通过构建n-gram来改进雅可比迭代的方法。

### 预期！
- 通过“缓存轨迹”来构建n-gram。

### 减少延迟！（单GPU结果）
- 展示了在不同任务上如何减少延迟。

### 总结
- 讨论了PABEE和CALM模型的早期退出策略，以及如何通过分页注意力和Flash-Decoding来提高LLM推理的效率。

### 讨论！
- 邀请学生和参与者就LLM推理的效率进行讨论。

这份PPT为学生提供了大型语言模型高效推理的全面介绍，包括早期退出策略、并行推理技术、优化的注意力算法以及内存管理技术。通过这些笔记，学生可以更好地理解如何提高LLM在推理阶段的性能，特别是在延迟和内存使用方面。