# Lecture12

这份PPT是关于压缩和稀疏化大型语言模型（LLM）的课程介绍，由Harrum Noor和Marium Aslam在2024年3月26日为加拿大滑铁卢大学（UWaterloo）的CS886课程准备。以下是对PPT内容的详细解释和分析，以及相应的课程笔记：

### 稀疏化简介
- **稀疏性**：利用条件计算的概念，仅在需要时运行系统的一部分，以提高效率、速度，降低能耗，并生成能力。

### 传统变换器架构
- **注意力机制**：允许模型考虑输入数据的全部上下文，与传统的序列模型相比是一个巨大的进步。

### 瓶颈问题
- 随着规模的扩大，变换器需要处理更多数据和复杂问题，导致资源密集和效率低下。

### 混合专家（MoE）
- 用MoE层替换密集的前馈层，通过路由机制决定输入令牌去往哪个专家。

### 为什么使用MoE？
- 分割模型以执行专门化任务，提高计算效率。

### 路由机制
- 学习机制，随时间改进，使专家专业化，从而在推理时减少计算量。

### MoE的效率
- MoE与密集模型相比，在计算上更高效，尤其是在领域内评估时。

### 实验
- 训练与GPT-3大小和架构相似的自回归变换器模型。

### 预训练
- 使用六个英语语言数据集的并集进行预训练。

### 评估语言模型：困惑度和性能
- 困惑度：衡量语言模型预测文本的能力。
- 下游任务性能：困惑度低并不一定意味着在实际应用中表现更好。

### 结果
- MoE模型在计算成本相当的情况下，无论是在领域内还是领域外数据上，都显示出比密集模型更低的困惑度。

### 扩展规模
- 神经网络的扩展可以带来显著的质量提升。

### GShard
- 谷歌构建了一个600亿参数的变换器，用于大规模多语言机器翻译。

### GShard架构
- 基于SPMD（单程序多数据）并行计算框架。

### 实施GShard
- 使用注释API添加注释，编译器生成在所有设备上并行执行的单一程序。

### XLA编译器
- 作为各种机器学习框架的通用后端，处理大型计算的分割和数据通信。

### 数据集和基线
- 网络规模数据集，包含100种语言的平行文档，用于训练。

### 模型变体
- 探索变换器编码器-解码器堆栈中的层数和每层MoE使用的专家总数。

### 结果
- 更深的模型和更多的专家可以提高所有语言的质量。

### 训练效率
- 更深的模型更样本高效，由于过参数化的加速效应，用更少的样本更快地收敛。

### 性能
- 最大模型（600B）可以在4天内完成训练，达到最佳质量。

### 内存消耗
- GShard的内存消耗来自复制权重、分布式权重和激活。

### GShard关键要点
- SPMD框架用于大规模扩展，提供注释API，使用XLA编译器。

### CoLT5：条件长T5
- 处理长输入的变换器模型，通过条件计算提高效率。

### CoLT5直觉
- 通过结合注意力和前馈层的架构改进，实现长输入的快速处理。

### 架构
- 全注意力与局部注意力的结合。

### CoLT5条件计算
- 包括路由机制、条件前馈和条件注意力三个组件。

### 实验：训练
- 在C4数据集上预训练，使用不同的批量大小和输入长度。

### 实验：数据集
- 使用不同的数据集进行训练和微调。

### 结果
- CoLT5在所有版本中每个样本的时间更少，实现了SCROLLS基准测试的最新性能。

### 缩放至极长输入
- CoLT5有效地扩展到极长输入，比LONGT5具有更强的性能和更快的速度。

### 消融研究
- CoLT5从动态路由令牌中受益，学习识别重要令牌并为其提供更多的处理能力。

### 限制
- CoLT5仅在编码器中应用条件计算，专门用于长序列，需要从头开始训练。

### 比较
- 比较了MoE、GShard和CoLT5的不同特征，包括规模、稀疏化技术、专家选择、注意力和专业化。

### 量化大型语言模型
- 讨论了量化的概念、原因和不同技术，如均匀和非均匀量化、动态量化、块量化和混合精度训练。

### 研究论文
- 介绍了几篇关于量化的论文，包括LLM.int8()、8-bit Optimizers via Block-wise Quantization、QLoRA和BitNet。

### LLM.int8()
- 介绍了一种新的量化方法，用于在不降低性能的情况下，将推理时间减半。

### 矩阵乘法：前馈层和注意力层
- 讨论了矩阵乘法在注意力层和前馈层中的作用。

### 8位量化：混合精度量化和向量量化
- 介绍了如何通过混合精度分解和向量量化来优化矩阵乘法操作。

### LLM.int8()结果
- 展示了C4困惑度和时间复杂度的结果。

### 8位优化器通过块量化
- 使用8位统计信息来维护32位优化器状态的性能水平。

### QLoRA：量化LLM的高效微调
- QLoRA通过减少平均内存需求，使得65B参数模型的微调从>780GB GPU内存减少到<48GB。

### BitNet：为大型语言模型扩展1位变换器
- BitNet通过使用1位权重的变换器架构，为大型语言模型提供了一种可扩展且稳定的解决方案。

### BitNet关键创新
- 量化感知训练、优化技术、BitLinear层和模型并行性。

### BitNet：BitLinear
- 使用BitLinear层替换nn.Linear层，直接从零开始训练1位权重。

### BitNet：模型训练
- 使用直通估计器、混合精度训练和大学习率来训练模型。

### BitNet：能耗
- 与传统变换器相比，BitNet在能耗上的优势。

### BitNet：下游任务结果
- BitNet在下游任务上的性能。

### BitNet：与FP16变换器的比较
- BitNet与传统全精度变换器的比较。

### BitNet：关键要点
- BitNet在大大减少内存和能源需求的同时，实现了与全精度模型相当的性能和任务成果。

### 量化比较表
- 比较了LLM.int8()、8-bit Optimizers、QLoRA和BitNet的不同特征和技术。

这份PPT为学生提供了大型语言模型压缩和稀疏化的全面介绍，包括稀疏性的概念、混合专家模型、GShard架构、CoLT5模型、量化技术以及这些技术如何帮助提高大型语言模型的效率和可扩展性。通过这些笔记，学生可以更好地理解如何通过稀疏化和量化技术来优化大型语言模型，以及这些技术在实际应用中的潜在影响。